{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 253367,
     "status": "ok",
     "timestamp": 1732691964262,
     "user": {
      "displayName": "Lim Jey",
      "userId": "15661344898352749079"
     },
     "user_tz": -540
    },
    "id": "q_Q3nu3GeFxr",
    "outputId": "b2afa24d-bf9f-4d55-dfee-f89dfc062549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-27 07:15:11--  https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
      "Resolving github.com (github.com)... 20.27.177.113\n",
      "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241127T071512Z&X-Amz-Expires=300&X-Amz-Signature=c619dfba67f22ecb3ef9ded7c75396efc495a4eb485f80041313de3a79f023cd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-11-27 07:15:12--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241127T071512Z&X-Amz-Expires=300&X-Amz-Signature=c619dfba67f22ecb3ef9ded7c75396efc495a4eb485f80041313de3a79f023cd&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38741381 (37M) [application/octet-stream]\n",
      "Saving to: ‘balloon_dataset.zip’\n",
      "\n",
      "balloon_dataset.zip 100%[===================>]  36.95M  47.3MB/s    in 0.8s    \n",
      "\n",
      "2024-11-27 07:15:13 (47.3 MB/s) - ‘balloon_dataset.zip’ saved [38741381/38741381]\n",
      "\n",
      "Archive:  /content/balloon_dataset.zip\n",
      "   creating: /content/balloon_dataset/balloon/\n",
      "   creating: /content/balloon_dataset/balloon/train/\n",
      "  inflating: /content/balloon_dataset/balloon/train/via_region_data.json  \n",
      "   creating: /content/balloon_dataset/__MACOSX/\n",
      "   creating: /content/balloon_dataset/__MACOSX/balloon/\n",
      "   creating: /content/balloon_dataset/__MACOSX/balloon/train/\n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._via_region_data.json  \n",
      "  inflating: /content/balloon_dataset/balloon/train/53500107_d24b11b3c2_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._53500107_d24b11b3c2_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/120853323_d4788431b9_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._120853323_d4788431b9_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/126700562_8e27720147_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._126700562_8e27720147_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/145053828_e0e748717c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._145053828_e0e748717c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/154446334_5d41cd1375_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._154446334_5d41cd1375_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/155815494_800fc9aa32_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._155815494_800fc9aa32_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/321888854_3723b6f10b_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._321888854_3723b6f10b_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/332344155_71be3a3b22_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._332344155_71be3a3b22_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/351678851_e2aeebdafd_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._351678851_e2aeebdafd_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/485227412_e335662bb5_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._485227412_e335662bb5_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/489752654_777853a0ba_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._489752654_777853a0ba_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/518678836_94d58e3839_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._518678836_94d58e3839_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/605521662_a470fef77f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._605521662_a470fef77f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/699765866_abaad7274d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._699765866_abaad7274d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/873768102_7d931e5fa3_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._873768102_7d931e5fa3_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/1297451346_5b92bdac08_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._1297451346_5b92bdac08_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/2311771643_f46392fcc0_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._2311771643_f46392fcc0_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/2354829160_3f65a6bf6f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._2354829160_3f65a6bf6f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/2385899600_94b68350af_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._2385899600_94b68350af_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/2685563244_b0d5f7eb67_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._2685563244_b0d5f7eb67_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/2937599387_80e7d6e050_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._2937599387_80e7d6e050_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/3342804367_f43682bb80_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._3342804367_f43682bb80_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/3646097131_e3e1215843_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._3646097131_e3e1215843_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/3927754171_9011487133_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._3927754171_9011487133_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/3945575930_ce99a7e98d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._3945575930_ce99a7e98d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4057490235_2ffdf7d68b_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4057490235_2ffdf7d68b_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4543126482_92254ef046_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4543126482_92254ef046_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4552737035_3a0a4105fb_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4552737035_3a0a4105fb_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4864857993_edb62f16ef_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4864857993_edb62f16ef_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4887227769_acd2e6127d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4887227769_acd2e6127d_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/4955354786_337a598e4a_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._4955354786_337a598e4a_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/5013250607_26359229b6_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._5013250607_26359229b6_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/5178670692_63a4365c9c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._5178670692_63a4365c9c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/5253122239_38b1e7f61c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._5253122239_38b1e7f61c_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/5560377994_cb597a4af5_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._5560377994_cb597a4af5_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/5674044810_2d9e2243ff_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._5674044810_2d9e2243ff_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/6483318883_21facf57cd_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._6483318883_21facf57cd_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/7178882742_f090f3ce56_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._7178882742_f090f3ce56_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/7308740338_591f27b631_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._7308740338_591f27b631_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/7488015492_0583857ca0_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._7488015492_0583857ca0_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/8436015314_3a678c1005_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._8436015314_3a678c1005_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/8758393087_76fcd56bd3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._8758393087_76fcd56bd3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/9210739293_2b0e0d991e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._9210739293_2b0e0d991e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/9330497995_4cf0438cb6_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._9330497995_4cf0438cb6_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/10464445726_6f1e3bbe6a_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._10464445726_6f1e3bbe6a_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/12037308314_e16fb3a0f7_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._12037308314_e16fb3a0f7_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/12288043903_fe1ea17a4e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._12288043903_fe1ea17a4e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/12288355124_5e340d3de3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._12288355124_5e340d3de3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/12288446656_2c6a90e6f5_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._12288446656_2c6a90e6f5_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/14321263043_b76ef054d3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._14321263043_b76ef054d3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/14666848163_8be8e37562_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._14666848163_8be8e37562_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/15290896925_884ab33fd3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._15290896925_884ab33fd3_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/15331928994_d5b82eb368_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._15331928994_d5b82eb368_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/15717689633_5f7f78c28e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._15717689633_5f7f78c28e_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/16435593892_2aa8118f4a_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._16435593892_2aa8118f4a_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/17156759330_5af4f5a5b8_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._17156759330_5af4f5a5b8_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/17178818589_16e58fc1e5_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._17178818589_16e58fc1e5_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/18849792632_aad23ad513_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._18849792632_aad23ad513_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/24362039530_b151b41a52_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._24362039530_b151b41a52_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/25899693952_7c8b8b9edc_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._25899693952_7c8b8b9edc_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/train/34020010494_e5cb88e1c4_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/train/._34020010494_e5cb88e1c4_k.jpg  \n",
      "   creating: /content/balloon_dataset/balloon/val/\n",
      "  inflating: /content/balloon_dataset/balloon/val/24631331976_defa3bb61f_k.jpg  \n",
      "   creating: /content/balloon_dataset/__MACOSX/balloon/val/\n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._24631331976_defa3bb61f_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/16335852991_f55de7958d_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._16335852991_f55de7958d_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/14898532020_ba6199dd22_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._14898532020_ba6199dd22_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/8053085540_a72bd21a64_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._8053085540_a72bd21a64_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/3800636873_ace2c2795f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._3800636873_ace2c2795f_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/3825919971_93fb1ec581_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._3825919971_93fb1ec581_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/4838031651_3e7b5ea5c7_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._4838031651_3e7b5ea5c7_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/410488422_5f8991f26e_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._410488422_5f8991f26e_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/6810773040_3d81036d05_k.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._6810773040_3d81036d05_k.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/5555705118_3390d70abe_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._5555705118_3390d70abe_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/5603212091_2dfe16ea72_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._5603212091_2dfe16ea72_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/via_region_data.json  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._via_region_data.json  \n",
      "  inflating: /content/balloon_dataset/balloon/val/4581425993_72b9b15fc0_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._4581425993_72b9b15fc0_b.jpg  \n",
      "  inflating: /content/balloon_dataset/balloon/val/2917282960_06beee649a_b.jpg  \n",
      "  inflating: /content/balloon_dataset/__MACOSX/balloon/val/._2917282960_06beee649a_b.jpg  \n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-35613tzd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-35613tzd\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit c69939aa85460e8135f40bce908a6cddaa73065f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (11.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.8.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.5.0)\n",
      "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.1.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.6)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.1)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black (from detectron2==0.6)\n",
      "  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (4.25.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6355037 sha256=e0d42b5f42a019f4a042aa642234404914c23eb616f99dec6a2ea164006e8b89\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3d1munt/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=2d85178d35b432abf61a433c7ebec46fb8c7c657bbc87b9c292c3d6e2acb70be\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=71c4ba5f8f24be5ca67a4e90d581f556bb61af992e8a943ed5688100f058c535\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 black-24.10.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.0.0 yacs-0.1.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "fdb39c6128ab4f03bfe8ddf506328e26",
       "pip_warning": {
        "packages": [
         "pydevd_plugins"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " \n",
    "# 예시 이미지와 풍선 데이터셋 다운로드 \n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O img_for_P3.jpg\n",
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
    "!unzip /content/balloon_dataset.zip -d /content/balloon_dataset\n",
    "\n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8507,
     "status": "ok",
     "timestamp": 1732697618205,
     "user": {
      "displayName": "Lim Jey",
      "userId": "15661344898352749079"
     },
     "user_tz": -540
    },
    "id": "Oxxfp0wjeJob",
    "outputId": "aa1aeca6-8f44-43ec-a43c-5e5f268cc136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.5 ; cuda:  cu121\n",
      "[11/27 08:53:29 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# PyTorch & CUDA 버전 확인\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "# 풍선 데이터셋 다운로드 및 압축 해제 후 데이터 경로 설정\n",
    "balloon_data_path = \"/content/balloon_dataset/balloon\"\n",
    "example_image_path = \"/content/img_for_P3.jpg\"\n",
    "output_path = \"/content/output_images\"\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# P3-1: 모델 설정\n",
    "model_name = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"  # 선택한 모델\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # 임계값 설정\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Predictor 초기화\n",
    "# DefaultPredictor:Detectron2에서 간편하게 모델을 실행할 수 있도록 설정된 추론 도구.\n",
    "# cfg 객체에 기반하여 모델을 초기화합니다.\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# 예시 이미지에 모델 적용 및 결과 확인\n",
    "example_img = cv2.imread(example_image_path)\n",
    "outputs = predictor(example_img)\n",
    "v = Visualizer(example_img[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# 예시 이미지 결과 저장\n",
    "cv2.imwrite(os.path.join(output_path, \"example_output.jpg\"), out.get_image()[:, :, ::-1])\n",
    "\n",
    "# 수정하지말것\n",
    "# 풍선 데이터셋 전처리 함수\n",
    "def get_balloon_dicts(img_dir):\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annos.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "# 데이터셋 등록\n",
    "for d in [\"train\", \"val\"]:\n",
    "    dataset_name = f\"balloon_{d}\"           #ballon_train으로 데이터셋 이름저장\n",
    "    if dataset_name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(dataset_name)  # 기존 데이터셋 제거\n",
    "    DatasetCatalog.register(dataset_name, lambda d=d: get_balloon_dicts(os.path.join(balloon_data_path, d)))\n",
    "    MetadataCatalog.get(dataset_name).set(thing_classes=[\"balloon\"])\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# 풍선 데이터셋 경로를 설정합니다.\n",
    "balloon_data_path = \"/content/balloon_dataset/balloon\"\n",
    "\n",
    "# 풍선 데이터셋의 학습 데이터셋 로드\n",
    "dataset_dicts = get_balloon_dicts(os.path.join(balloon_data_path, \"train\"))\n",
    "\n",
    "# 모델 적용 및 결과 저장\n",
    "random.seed(1234)\n",
    "for idx, d in enumerate(random.sample(dataset_dicts, 15)):  # 15개의 샘플에 대해 실행\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    # Get the predicted classes and map them to the balloon dataset class\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    pred_classes = instances.pred_classes.tolist()\n",
    "\n",
    "    # Assuming 'balloon' is class 0 in your balloon dataset\n",
    "    # Map all predicted classes to 0 (balloon)\n",
    "    # This assumes your balloon dataset only has one class 'balloon'\n",
    "    pred_classes = [0] * len(pred_classes)\n",
    "    instances.pred_classes = torch.tensor(pred_classes, device=\"cpu\")\n",
    "\n",
    "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"balloon_train\"), scale=0.5)\n",
    "    out = v.draw_instance_predictions(instances) # Use the modified instances\n",
    "    output_image_path = f\"/content/output_images/balloon_output_{idx}.jpg\"\n",
    "    cv2.imwrite(output_image_path, out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139752,
     "status": "ok",
     "timestamp": 1732697770774,
     "user": {
      "displayName": "Lim Jey",
      "userId": "15661344898352749079"
     },
     "user_tz": -540
    },
    "id": "WNvzOk0qeJxK",
    "outputId": "abea4c9d-d271-4f32-ab13-3574ecd9faf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/27 08:53:51 d2.engine.defaults]: Model:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[11/27 08:53:51 d2.data.build]: Removed 0 images with no usable annotations. 61 images left.\n",
      "[11/27 08:53:51 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "[11/27 08:53:51 d2.data.build]: Using training sampler TrainingSampler\n",
      "[11/27 08:53:51 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[11/27 08:53:51 d2.data.common]: Serializing 61 elements to byte tensors and concatenating them all ...\n",
      "[11/27 08:53:51 d2.data.common]: Serialized dataset takes 0.17 MiB\n",
      "[11/27 08:53:51 d2.data.build]: Making batched data loader with batch_size=2\n",
      "WARNING [11/27 08:53:51 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "[11/27 08:53:51 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
      "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
      "roi_heads.box_predictor.cls_score.{bias, weight}\n",
      "roi_heads.mask_head.predictor.{bias, weight}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/27 08:53:52 d2.engine.train_loop]: Starting training from iteration 0\n",
      "[11/27 08:54:00 d2.utils.events]:  eta: 0:01:49  iter: 19  total_loss: 2.058  loss_cls: 0.6973  loss_box_reg: 0.6459  loss_mask: 0.6787  loss_rpn_cls: 0.03607  loss_rpn_loc: 0.01281    time: 0.3961  last_time: 0.4385  data_time: 0.0197  last_data_time: 0.0065   lr: 1.6068e-05  max_mem: 4875M\n",
      "[11/27 08:54:08 d2.utils.events]:  eta: 0:01:43  iter: 39  total_loss: 1.746  loss_cls: 0.5797  loss_box_reg: 0.5416  loss_mask: 0.5854  loss_rpn_cls: 0.05288  loss_rpn_loc: 0.004136    time: 0.4042  last_time: 0.4019  data_time: 0.0053  last_data_time: 0.0091   lr: 3.2718e-05  max_mem: 4875M\n",
      "[11/27 08:54:17 d2.utils.events]:  eta: 0:01:40  iter: 59  total_loss: 1.603  loss_cls: 0.4459  loss_box_reg: 0.6928  loss_mask: 0.4814  loss_rpn_cls: 0.01914  loss_rpn_loc: 0.007977    time: 0.4145  last_time: 0.4524  data_time: 0.0050  last_data_time: 0.0042   lr: 4.9367e-05  max_mem: 4878M\n",
      "[11/27 08:54:25 d2.utils.events]:  eta: 0:01:32  iter: 79  total_loss: 1.503  loss_cls: 0.3783  loss_box_reg: 0.6175  loss_mask: 0.3737  loss_rpn_cls: 0.04453  loss_rpn_loc: 0.007875    time: 0.4186  last_time: 0.3829  data_time: 0.0052  last_data_time: 0.0042   lr: 6.6017e-05  max_mem: 4878M\n",
      "[11/27 08:54:34 d2.utils.events]:  eta: 0:01:24  iter: 99  total_loss: 1.247  loss_cls: 0.289  loss_box_reg: 0.6197  loss_mask: 0.2658  loss_rpn_cls: 0.01544  loss_rpn_loc: 0.004384    time: 0.4206  last_time: 0.4287  data_time: 0.0050  last_data_time: 0.0058   lr: 8.2668e-05  max_mem: 4878M\n",
      "[11/27 08:54:43 d2.utils.events]:  eta: 0:01:17  iter: 119  total_loss: 1.203  loss_cls: 0.2593  loss_box_reg: 0.6538  loss_mask: 0.2372  loss_rpn_cls: 0.02767  loss_rpn_loc: 0.008565    time: 0.4229  last_time: 0.4281  data_time: 0.0050  last_data_time: 0.0031   lr: 9.9318e-05  max_mem: 5087M\n",
      "[11/27 08:54:51 d2.utils.events]:  eta: 0:01:08  iter: 139  total_loss: 1.038  loss_cls: 0.1975  loss_box_reg: 0.5682  loss_mask: 0.1769  loss_rpn_cls: 0.02958  loss_rpn_loc: 0.009663    time: 0.4234  last_time: 0.4018  data_time: 0.0048  last_data_time: 0.0031   lr: 0.00011597  max_mem: 5087M\n",
      "[11/27 08:55:00 d2.utils.events]:  eta: 0:01:00  iter: 159  total_loss: 0.9114  loss_cls: 0.1421  loss_box_reg: 0.5952  loss_mask: 0.1394  loss_rpn_cls: 0.01389  loss_rpn_loc: 0.00724    time: 0.4240  last_time: 0.4588  data_time: 0.0054  last_data_time: 0.0039   lr: 0.00013262  max_mem: 5087M\n",
      "[11/27 08:55:08 d2.utils.events]:  eta: 0:00:51  iter: 179  total_loss: 0.7379  loss_cls: 0.1321  loss_box_reg: 0.4557  loss_mask: 0.1318  loss_rpn_cls: 0.0183  loss_rpn_loc: 0.008854    time: 0.4231  last_time: 0.4748  data_time: 0.0052  last_data_time: 0.0030   lr: 0.00014927  max_mem: 5087M\n",
      "[11/27 08:55:17 d2.utils.events]:  eta: 0:00:42  iter: 199  total_loss: 0.5564  loss_cls: 0.09603  loss_box_reg: 0.2971  loss_mask: 0.1124  loss_rpn_cls: 0.01865  loss_rpn_loc: 0.01132    time: 0.4255  last_time: 0.4987  data_time: 0.0056  last_data_time: 0.0076   lr: 0.00016592  max_mem: 5087M\n",
      "[11/27 08:55:26 d2.utils.events]:  eta: 0:00:34  iter: 219  total_loss: 0.4715  loss_cls: 0.08099  loss_box_reg: 0.2567  loss_mask: 0.08108  loss_rpn_cls: 0.01752  loss_rpn_loc: 0.005847    time: 0.4265  last_time: 0.4598  data_time: 0.0053  last_data_time: 0.0037   lr: 0.00018257  max_mem: 5087M\n",
      "[11/27 08:55:35 d2.utils.events]:  eta: 0:00:25  iter: 239  total_loss: 0.3838  loss_cls: 0.09053  loss_box_reg: 0.1952  loss_mask: 0.08842  loss_rpn_cls: 0.01645  loss_rpn_loc: 0.009281    time: 0.4295  last_time: 0.4561  data_time: 0.0050  last_data_time: 0.0049   lr: 0.00019922  max_mem: 5087M\n",
      "[11/27 08:55:44 d2.utils.events]:  eta: 0:00:17  iter: 259  total_loss: 0.3498  loss_cls: 0.07426  loss_box_reg: 0.1804  loss_mask: 0.07352  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.006346    time: 0.4303  last_time: 0.4226  data_time: 0.0051  last_data_time: 0.0011   lr: 0.00021587  max_mem: 5087M\n",
      "[11/27 08:55:53 d2.utils.events]:  eta: 0:00:08  iter: 279  total_loss: 0.3781  loss_cls: 0.07757  loss_box_reg: 0.1855  loss_mask: 0.1012  loss_rpn_cls: 0.009243  loss_rpn_loc: 0.005415    time: 0.4311  last_time: 0.4505  data_time: 0.0050  last_data_time: 0.0051   lr: 0.00023252  max_mem: 5087M\n",
      "[11/27 08:56:02 d2.utils.events]:  eta: 0:00:00  iter: 299  total_loss: 0.3454  loss_cls: 0.0704  loss_box_reg: 0.1638  loss_mask: 0.07557  loss_rpn_cls: 0.004851  loss_rpn_loc: 0.006985    time: 0.4314  last_time: 0.4118  data_time: 0.0050  last_data_time: 0.0042   lr: 0.00024917  max_mem: 5087M\n",
      "[11/27 08:56:03 d2.engine.hooks]: Overall training speed: 298 iterations in 0:02:08 (0.4314 s / it)\n",
      "[11/27 08:56:03 d2.engine.hooks]: Total training time: 0:02:10 (0:00:01 on hooks)\n",
      "[11/27 08:56:03 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[11/27 08:56:03 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[11/27 08:56:03 d2.data.common]: Serializing 13 elements to byte tensors and concatenating them all ...\n",
      "[11/27 08:56:03 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "WARNING [11/27 08:56:03 d2.engine.defaults]: No evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "WARNING [11/27 08:56:03 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "[11/27 08:56:04 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "[11/27 08:56:04 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "[11/27 08:56:04 d2.data.common]: Serializing 13 elements to byte tensors and concatenating them all ...\n",
      "[11/27 08:56:04 d2.data.common]: Serialized dataset takes 0.04 MiB\n",
      "[11/27 08:56:04 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n",
      "[11/27 08:56:05 d2.evaluation.evaluator]: Start inference on 13 batches\n",
      "[11/27 08:56:06 d2.evaluation.evaluator]: Inference done 11/13. Dataloading: 0.0013 s/iter. Inference: 0.1083 s/iter. Eval: 0.0048 s/iter. Total: 0.1143 s/iter. ETA=0:00:00\n",
      "[11/27 08:56:07 d2.evaluation.evaluator]: Total inference time: 0:00:00.980398 (0.122550 s / iter per device, on 1 devices)\n",
      "[11/27 08:56:07 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:00 (0.108570 s / iter per device, on 1 devices)\n",
      "[11/27 08:56:07 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[11/27 08:56:07 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
      "[11/27 08:56:07 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.720\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.838\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.800\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.532\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.888\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.232\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.736\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.736\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.907\n",
      "[11/27 08:56:07 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 71.956 | 83.811 | 80.005 | 0.000 | 53.203 | 88.850 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[11/27 08:56:07 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.755\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.809\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.809\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.528\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.956\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.252\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.772\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.772\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.970\n",
      "[11/27 08:56:07 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 75.542 | 80.927 | 80.927 | 0.000 | 52.782 | 95.571 |\n",
      "Evaluation Results (Fine-tuned Model):\n",
      "OrderedDict([('bbox', {'AP': 71.95592029875974, 'AP50': 83.81123826668382, 'AP75': 80.00482975126782, 'APs': 0.0, 'APm': 53.20290600488621, 'APl': 88.84996415274283}), ('segm', {'AP': 75.54208158387391, 'AP50': 80.92742201049374, 'AP75': 80.92742201049374, 'APs': 0.0, 'APm': 52.78153529638677, 'APl': 95.5708965127282})])\n",
      "[11/27 08:56:07 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
      "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
      "roi_heads.box_predictor.cls_score.{bias, weight}\n",
      "roi_heads.mask_head.predictor.{bias, weight}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING [11/27 08:56:07 d2.evaluation.coco_evaluation]: COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "[11/27 08:56:07 d2.evaluation.evaluator]: Start inference on 13 batches\n",
      "[11/27 08:56:09 d2.evaluation.evaluator]: Total inference time: 0:00:00.986857 (0.123357 s / iter per device, on 1 devices)\n",
      "[11/27 08:56:09 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:00 (0.108245 s / iter per device, on 1 devices)\n",
      "[11/27 08:56:09 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
      "[11/27 08:56:09 d2.evaluation.coco_evaluation]: Saving results to ./output_pretrained/coco_instances_results.json\n",
      "[11/27 08:56:09 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.00 seconds.\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "[11/27 08:56:09 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: Evaluate annotation type *segm*\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
      "[11/27 08:56:09 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "[11/27 08:56:09 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n",
      "Evaluation Results (Pre-trained Model):\n",
      "OrderedDict([('bbox', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0}), ('segm', {'AP': 0.0, 'AP50': 0.0, 'AP75': 0.0, 'APs': 0.0, 'APm': 0.0, 'APl': 0.0})])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# P3-2: 파인튜닝\n",
    "# 파인튜닝을 위한 config 설정\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
    "cfg.DATASETS.TRAIN = (\"balloon_train\",)\n",
    "cfg.DATASETS.TEST = (\"balloon_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)  # 사전 학습 모델로 초기화\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # 배치 크기\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # 학습률\n",
    "cfg.SOLVER.MAX_ITER = 300   # 학습 반복 횟수\n",
    "cfg.SOLVER.STEPS = [500, 750]  # 학습률 감소 단계\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # RoI 헤드 배치 크기\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 클래스 수 (풍선 하나)\n",
    "# Explicitly set the device to \"cpu\" if no CUDA is available\n",
    "#cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 출력 디렉토리 생성\n",
    "# cfg.OUTPUT_DIR을 명시적으로 설정하지 않았다면, 기본적으로 모델 저장 경로는 현재 작업 디렉토리의 output\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Trainer 초기화 및 학습\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# COCOEvaluator를 사용하여 모델 성능 평가\n",
    "evaluator = COCOEvaluator(\"balloon_val\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"balloon_val\")\n",
    "\n",
    "# P3-3: 모델 검증\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # 학습 완료된 모델\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # 테스트 시 임계값\n",
    "predictor = DefaultPredictor(cfg)\n",
    "# 파인튜닝 후 성능 평가\n",
    "evaluation_results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "print(\"Evaluation Results (Fine-tuned Model):\")\n",
    "print(evaluation_results)\n",
    "\n",
    "# 파인튜닝 이전 모델 성능 평가 (사전 학습 모델)\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)        # 사전 학습 모델로 초기화 \n",
    "predictor_pretrained = DefaultPredictor(cfg)\n",
    "evaluator_pretrained = COCOEvaluator(\"balloon_val\", cfg, False, output_dir=\"./output_pretrained/\")\n",
    "evaluation_results_pretrained = inference_on_dataset(predictor_pretrained.model, val_loader, evaluator_pretrained)\n",
    "print(\"Evaluation Results (Pre-trained Model):\")\n",
    "print(evaluation_results_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1732697776515,
     "user": {
      "displayName": "Lim Jey",
      "userId": "15661344898352749079"
     },
     "user_tz": -540
    },
    "id": "KvziA09UeJ5w",
    "outputId": "e6f6c24b-2eea-4b45-9bc8-acc4a73bd0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/27 08:56:14 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from ./output/model_final.pth ...\n",
      "Fine-tuned model applied result saved at: /content/output_images/fine_tuned_example_output.jpg\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝된 모델 설정\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # 학습된 모델 가중치\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # 테스트 시 임계값\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# 3-1의 예시 이미지에 파인튜닝된 모델 적용\n",
    "example_img = cv2.imread(example_image_path)\n",
    "outputs = predictor(example_img)\n",
    "\n",
    "# 시각화 및 결과 저장\n",
    "v = Visualizer(example_img[:, :, ::-1], MetadataCatalog.get(\"balloon_train\"), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# 저장된 결과 확인\n",
    "output_example_path = os.path.join(output_path, \"fine_tuned_example_output.jpg\")\n",
    "cv2.imwrite(output_example_path, out.get_image()[:, :, ::-1])\n",
    "print(f\"Fine-tuned model applied result saved at: {output_example_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPTayMaTb16tseq0Ti4eVlA",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
